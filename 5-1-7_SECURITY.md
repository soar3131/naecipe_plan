# ë‚´ì‹œí”¼(Naecipe) ë³´ì•ˆ ë° í’ˆì§ˆ

> ìƒìœ„ ë¬¸ì„œ: [5-1SERVICE_ARCHITECTURE.md](./5-1SERVICE_ARCHITECTURE.md)

---

## 1. ë³´ì•ˆ ì•„í‚¤í…ì²˜

### 1.1 3ê³„ì¸µ ë³´ì•ˆ ëª¨ë¸

```mermaid
flowchart TB
    subgraph Perimeter["ê²½ê³„ ë³´ì•ˆ (Perimeter)"]
        WAF[AWS WAF]
        SHIELD[AWS Shield]
        CF[CloudFront]
    end

    subgraph Network["ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ (Network)"]
        VPC[VPC]
        SG[Security Groups]
        NACL[Network ACLs]
        TLS[TLS 1.3]
    end

    subgraph Application["ì• í”Œë¦¬ì¼€ì´ì…˜ ë³´ì•ˆ (Application)"]
        AUTH[ì¸ì¦/ì¸ê°€]
        INPUT[ì…ë ¥ ê²€ì¦]
        ENCRYPT[ì•”í˜¸í™”]
        AUDIT[ê°ì‚¬ ë¡œê·¸]
    end

    subgraph Data["ë°ì´í„° ë³´ì•ˆ (Data)"]
        RDS_ENC[RDS ì•”í˜¸í™”]
        S3_ENC[S3 ì•”í˜¸í™”]
        SECRETS[Secrets Manager]
        BACKUP[ì•”í˜¸í™”ëœ ë°±ì—…]
    end

    Perimeter --> Network
    Network --> Application
    Application --> Data
```

### 1.2 OWASP Top 10 ëŒ€ì‘

| ì·¨ì•½ì  | ëŒ€ì‘ ë°©ì•ˆ | êµ¬í˜„ |
|--------|----------|------|
| **A01 - Broken Access Control** | RBAC, ë¦¬ì†ŒìŠ¤ ì†Œìœ ê¶Œ ê²€ì¦ | Middleware + Policy |
| **A02 - Cryptographic Failures** | TLS 1.3, AES-256 ì•”í˜¸í™” | AWS KMS |
| **A03 - Injection** | Parameterized Query, Input Validation | SQLAlchemy + Pydantic |
| **A04 - Insecure Design** | Threat Modeling, Security Review | Architecture Review |
| **A05 - Security Misconfiguration** | IaC, Security Baseline | Terraform + CIS Benchmark |
| **A06 - Vulnerable Components** | Dependency Scanning, Auto-update | Dependabot + Snyk |
| **A07 - Auth Failures** | MFA, Rate Limiting, Secure Session | JWT + Redis |
| **A08 - Data Integrity Failures** | Signed Artifacts, Version Pinning | Cosign + SBOM |
| **A09 - Security Logging Failures** | Centralized Logging, Alerting | Loki + Alertmanager |
| **A10 - SSRF** | URL Allowlist, Request Validation | Middleware |

### 1.5 í¬ë¡¤ë§ ë³´ì•ˆ ì •ì±…

#### ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ë³´ì•ˆ

| ìœ„í˜‘ | ëŒ€ì‘ ë°©ì•ˆ | êµ¬í˜„ |
|------|----------|------|
| **ì•…ì„± ì½˜í…ì¸  ì£¼ì…** | LLM íŒŒì‹± ê²°ê³¼ ê²€ì¦, ì…ë ¥ Sanitization | Pydantic + íŒ¨í„´ ê²€ì‚¬ |
| **í”Œë«í¼ ì°¨ë‹¨** | Rate Limiting, User-Agent ë¡œí…Œì´ì…˜, ì¡´ì¤‘í•˜ëŠ” í¬ë¡¤ë§ | robots.txt ì¤€ìˆ˜ |
| **ì €ì‘ê¶Œ ì´ìŠˆ** | ì¶œì²˜ ëª…ì‹œ, ì›ë³¸ ë§í¬ ë³´ì¡´, ì‚­ì œ ìš”ì²­ ëŒ€ì‘ | ì†ŒìŠ¤ í…Œì´ë¸” ê´€ë¦¬ |
| **ë°ì´í„° í’ˆì§ˆ** | ì´ìƒì¹˜ íƒì§€, ìˆ˜ë™ ê²€ìˆ˜ í”Œë˜ê·¸ | í’ˆì§ˆ ì ìˆ˜ ì‹œìŠ¤í…œ |
| **API í‚¤ ë…¸ì¶œ** | í™˜ê²½ ë³€ìˆ˜, Secrets Manager | K8s Secrets |

#### Crawler Bot ë³´ì•ˆ ì„¤ì •

```python
# crawler/security.py

from pydantic import BaseModel, field_validator
import re
import bleach

class RecipeInputValidator(BaseModel):
    """í¬ë¡¤ë§ëœ ë ˆì‹œí”¼ ë°ì´í„° ê²€ì¦"""

    title: str
    description: str
    author_name: str
    ingredients: list[dict]
    steps: list[dict]

    @field_validator('title', 'description', 'author_name')
    @classmethod
    def sanitize_text(cls, v: str) -> str:
        # HTML íƒœê·¸ ì œê±°
        v = bleach.clean(v, tags=[], strip=True)
        # ìŠ¤í¬ë¦½íŠ¸ íŒ¨í„´ ê²€ì‚¬
        if re.search(r'<script|javascript:|on\w+=', v, re.IGNORECASE):
            raise ValueError('Potentially malicious content detected')
        # ê¸¸ì´ ì œí•œ
        if len(v) > 10000:
            raise ValueError('Content too long')
        return v.strip()

    @field_validator('ingredients', 'steps')
    @classmethod
    def validate_list_items(cls, v: list) -> list:
        if len(v) > 100:
            raise ValueError('Too many items')
        return v


class CrawlerRateLimiter:
    """í”Œë«í¼ë³„ Rate Limiting"""

    RATE_LIMITS = {
        'youtube': {'requests_per_minute': 30, 'delay_seconds': 2},
        'instagram': {'requests_per_minute': 20, 'delay_seconds': 3},
        'naver_blog': {'requests_per_minute': 10, 'delay_seconds': 6},
    }

    def __init__(self, platform: str):
        self.config = self.RATE_LIMITS.get(platform, {'delay_seconds': 5})

    async def wait(self):
        await asyncio.sleep(self.config['delay_seconds'])
```

#### robots.txt ì¤€ìˆ˜

```python
# crawler/robots.py

from urllib.robotparser import RobotFileParser
from functools import lru_cache

class RobotsChecker:
    """robots.txt ì¤€ìˆ˜ ê²€ì‚¬"""

    @lru_cache(maxsize=100)
    def _get_parser(self, base_url: str) -> RobotFileParser:
        rp = RobotFileParser()
        rp.set_url(f"{base_url}/robots.txt")
        rp.read()
        return rp

    def can_fetch(self, url: str) -> bool:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        parser = self._get_parser(base_url)
        return parser.can_fetch("NaecipeBot", url)
```

### 1.3 ì¸ì¦ ë³´ì•ˆ

```mermaid
sequenceDiagram
    autonumber
    participant User as ğŸ‘¤ User
    participant App as ğŸ“± App
    participant Gateway as ğŸšª Gateway
    participant Auth as ğŸ” Auth Service
    participant Redis as ğŸ’¾ Redis

    Note over User, Redis: ë¡œê·¸ì¸ í”Œë¡œìš°
    User->>App: ë¡œê·¸ì¸ ìš”ì²­
    App->>Gateway: POST /auth/login
    Gateway->>Auth: ì¸ì¦ ìš”ì²­
    Auth->>Auth: ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ (bcrypt)
    Auth->>Redis: ì„¸ì…˜ ì €ì¥
    Auth->>Auth: JWT ìƒì„± (RS256)
    Auth-->>Gateway: Access + Refresh Token
    Gateway-->>App: Set-Cookie (HttpOnly, Secure)
    App-->>User: ë¡œê·¸ì¸ ì„±ê³µ

    Note over User, Redis: API ìš”ì²­ í”Œë¡œìš°
    User->>App: API ìš”ì²­
    App->>Gateway: Request + Cookie
    Gateway->>Gateway: JWT ê²€ì¦
    Gateway->>Redis: ì„¸ì…˜ ìœ íš¨ì„± í™•ì¸
    Redis-->>Gateway: Valid
    Gateway->>Auth: ê¶Œí•œ í™•ì¸
    Auth-->>Gateway: Authorized
    Gateway->>Service: ìš”ì²­ ì „ë‹¬
```

### 1.4 ë°ì´í„° ì•”í˜¸í™”

| ë°ì´í„° ìœ í˜• | ì €ì¥ ì‹œ ì•”í˜¸í™” | ì „ì†¡ ì‹œ ì•”í˜¸í™” | í‚¤ ê´€ë¦¬ |
|------------|---------------|---------------|---------|
| **ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸** | bcrypt (cost 12) | TLS 1.3 | N/A |
| **ê°œì¸ì •ë³´ (ì´ë©”ì¼, ì´ë¦„)** | AES-256-GCM | TLS 1.3 | AWS KMS |
| **ì„¸ì…˜ í† í°** | N/A | TLS 1.3 | Redis Memory |
| **API í‚¤** | AES-256-GCM | TLS 1.3 | AWS Secrets Manager |
| **DB ë°ì´í„°** | RDS ì•”í˜¸í™” (AES-256) | TLS 1.3 | AWS KMS CMK |
| **ë°±ì—…** | AES-256 | N/A | AWS KMS CMK |

---

## 2. ì„±ëŠ¥ ìµœì í™”

### 2.1 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

#### ì¸ë±ìŠ¤ ì „ëµ

```sql
-- Recipe ê²€ìƒ‰ ìµœì í™”
CREATE INDEX idx_recipes_search ON recipes USING gin(
    to_tsvector('korean', title || ' ' || COALESCE(description, ''))
);

-- ë³µí•© ì¸ë±ìŠ¤: í•„í„°ë§ + ì •ë ¬
CREATE INDEX idx_recipes_filter_sort ON recipes(
    difficulty,
    cooking_time_minutes,
    created_at DESC
) WHERE is_active = true;

-- ì»¤ë²„ë§ ì¸ë±ìŠ¤: ëª©ë¡ ì¡°íšŒ
CREATE INDEX idx_recipes_list_covering ON recipes(
    id,
    title,
    thumbnail_url,
    cooking_time_minutes,
    difficulty,
    avg_rating
) WHERE is_active = true;

-- Cookbook ì¡°íšŒ ìµœì í™”
CREATE INDEX idx_cookbook_recipes_user ON cookbook_recipes(
    cookbook_id,
    created_at DESC
);

-- í”¼ë“œë°± ì¡°íšŒ ìµœì í™”
CREATE INDEX idx_feedbacks_recipe_date ON cooking_feedbacks(
    cookbook_recipe_id,
    created_at DESC
);
```

#### ì¿¼ë¦¬ ìµœì í™”

```sql
-- Before: N+1 ë¬¸ì œ
SELECT * FROM recipes WHERE id = $1;
-- Then for each recipe:
SELECT * FROM ingredients WHERE recipe_id = $1;
SELECT * FROM cooking_steps WHERE recipe_id = $1;

-- After: JOIN + JSON ì§‘ê³„
SELECT
    r.*,
    (
        SELECT json_agg(i ORDER BY i.order_index)
        FROM ingredients i
        WHERE i.recipe_id = r.id
    ) as ingredients,
    (
        SELECT json_agg(s ORDER BY s.step_number)
        FROM cooking_steps s
        WHERE s.recipe_id = r.id
    ) as steps
FROM recipes r
WHERE r.id = $1;
```

### 2.2 ìºì‹œ ìµœì í™”

```mermaid
flowchart TB
    subgraph Request["ìš”ì²­ ì²˜ë¦¬"]
        REQ[API ìš”ì²­]
    end

    subgraph L1["L1 Cache (In-Memory)"]
        LRU[LRU Cache<br/>Hot Data]
    end

    subgraph L2["L2 Cache (Redis)"]
        REDIS[(Redis Cluster)]
    end

    subgraph L3["L3 Cache (CDN)"]
        CDN[CloudFront]
    end

    subgraph DB["Database"]
        PG[(PostgreSQL)]
    end

    REQ --> L1
    L1 -->|Miss| L2
    L2 -->|Miss| PG
    PG -->|Response| L2
    L2 -->|Response| L1
    L1 -->|Response| REQ

    CDN -->|Static| REQ
```

### 2.3 í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”

| ìµœì í™” í•­ëª© | ì ìš© ê¸°ìˆ  | ëª©í‘œ |
|------------|----------|------|
| **ë²ˆë“¤ í¬ê¸°** | Tree Shaking, Code Splitting | < 200KB (gzipped) |
| **ì´ë¯¸ì§€** | Next.js Image, WebP, AVIF | LCP < 2.5s |
| **í°íŠ¸** | Font Subsetting, `font-display: swap` | FCP < 1.8s |
| **JavaScript** | Lazy Loading, Preload | TTI < 3.8s |
| **CSS** | Tailwind Purge, Critical CSS | CLS < 0.1 |

```typescript
// next.config.js - ë²ˆë“¤ ìµœì í™”

/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    optimizePackageImports: ['lucide-react', '@radix-ui/*'],
  },
  images: {
    formats: ['image/avif', 'image/webp'],
    deviceSizes: [640, 750, 828, 1080, 1200],
    imageSizes: [16, 32, 48, 64, 96, 128, 256],
  },
  webpack: (config, { isServer }) => {
    if (!isServer) {
      config.optimization.splitChunks = {
        chunks: 'all',
        cacheGroups: {
          default: false,
          vendors: false,
          framework: {
            name: 'framework',
            test: /[\\/]node_modules[\\/](react|react-dom|scheduler)[\\/]/,
            priority: 40,
            enforce: true,
          },
          lib: {
            test: /[\\/]node_modules[\\/]/,
            name(module) {
              const match = module.context.match(
                /[\\/]node_modules[\\/](.*?)([\\/]|$)/
              );
              return `npm.${match[1].replace('@', '')}`;
            },
            priority: 30,
            minChunks: 1,
            reuseExistingChunk: true,
          },
        },
      };
    }
    return config;
  },
};
```

---

## 3. í…ŒìŠ¤íŠ¸ ì „ëµ

### 3.1 í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

```mermaid
flowchart TB
    subgraph E2E["E2E Tests (10%)"]
        E2E1[Critical User Flows]
        E2E2[Cross-Service Integration]
    end

    subgraph Integration["Integration Tests (30%)"]
        INT1[API Tests]
        INT2[DB Integration]
        INT3[Cache Integration]
    end

    subgraph Unit["Unit Tests (60%)"]
        UNIT1[Business Logic]
        UNIT2[Utilities]
        UNIT3[Components]
    end

    E2E --> Integration
    Integration --> Unit
```

### 3.2 ì»¤ë²„ë¦¬ì§€ ëª©í‘œ

| í…ŒìŠ¤íŠ¸ ìœ í˜• | ì»¤ë²„ë¦¬ì§€ ëª©í‘œ | ë„êµ¬ |
|------------|--------------|------|
| **Unit Tests** | 80%+ | Vitest, Jest |
| **Integration Tests** | í•µì‹¬ API 100% | Supertest, TestContainers |
| **E2E Tests** | Critical Paths | Playwright |
| **Contract Tests** | ì„œë¹„ìŠ¤ ê°„ API | Pact |
| **Performance Tests** | ë¶€í•˜ í…ŒìŠ¤íŠ¸ | k6 |
| **Security Tests** | OWASP Top 10 | OWASP ZAP |

### 3.3 í…ŒìŠ¤íŠ¸ ì½”ë“œ ì˜ˆì‹œ

```python
# services/recipe-service/tests/test_recipes_service.py

import pytest
from unittest.mock import AsyncMock, MagicMock
from fastapi import HTTPException

from app.services.recipes_service import RecipesService
from app.models.recipe import Recipe
from app.cache.cache_service import CacheService
from app.search.search_service import SearchService


@pytest.fixture
def mock_repository():
    repo = AsyncMock()
    repo.find_by_id = AsyncMock()
    repo.find_all = AsyncMock()
    repo.save = AsyncMock()
    return repo


@pytest.fixture
def mock_cache_service():
    cache = AsyncMock(spec=CacheService)
    cache.get = AsyncMock()
    cache.set = AsyncMock()
    cache.delete = AsyncMock()
    return cache


@pytest.fixture
def mock_search_service():
    search = AsyncMock(spec=SearchService)
    search.search = AsyncMock()
    search.index = AsyncMock()
    return search


@pytest.fixture
def recipes_service(mock_repository, mock_cache_service, mock_search_service):
    return RecipesService(
        repository=mock_repository,
        cache_service=mock_cache_service,
        search_service=mock_search_service
    )


class TestRecipesService:

    @pytest.mark.asyncio
    async def test_find_by_id_returns_cached_recipe(
        self, recipes_service, mock_cache_service, mock_repository
    ):
        """ìºì‹œì— ë ˆì‹œí”¼ê°€ ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë°˜í™˜"""
        cached_recipe = {"id": "1", "title": "ê¹€ì¹˜ì°Œê°œ"}
        mock_cache_service.get.return_value = cached_recipe

        result = await recipes_service.find_by_id("1")

        mock_cache_service.get.assert_called_once_with("recipe:1")
        mock_repository.find_by_id.assert_not_called()
        assert result == cached_recipe

    @pytest.mark.asyncio
    async def test_find_by_id_fetches_from_db_on_cache_miss(
        self, recipes_service, mock_cache_service, mock_repository
    ):
        """ìºì‹œ ë¯¸ìŠ¤ ì‹œ DBì—ì„œ ì¡°íšŒ í›„ ìºì‹œì— ì €ì¥"""
        db_recipe = Recipe(id="1", title="ê¹€ì¹˜ì°Œê°œ", is_active=True)
        mock_cache_service.get.return_value = None
        mock_repository.find_by_id.return_value = db_recipe

        result = await recipes_service.find_by_id("1")

        mock_repository.find_by_id.assert_called_once_with(
            "1", include_relations=["ingredients", "steps", "tags"]
        )
        mock_cache_service.set.assert_called_once()
        assert result.id == "1"

    @pytest.mark.asyncio
    async def test_find_by_id_raises_not_found(
        self, recipes_service, mock_cache_service, mock_repository
    ):
        """ë ˆì‹œí”¼ê°€ ì—†ìœ¼ë©´ 404 ì—ëŸ¬"""
        mock_cache_service.get.return_value = None
        mock_repository.find_by_id.return_value = None

        with pytest.raises(HTTPException) as exc_info:
            await recipes_service.find_by_id("1")

        assert exc_info.value.status_code == 404

    @pytest.mark.asyncio
    async def test_search_with_filters(
        self, recipes_service, mock_search_service
    ):
        """í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        search_results = {
            "hits": [{"id": "1", "title": "ê¹€ì¹˜ì°Œê°œ"}],
            "total": 1,
        }
        mock_search_service.search.return_value = search_results

        result = await recipes_service.search(
            query="ê¹€ì¹˜",
            difficulty="easy",
            max_cooking_time=30
        )

        mock_search_service.search.assert_called_once()
        call_args = mock_search_service.search.call_args
        assert call_args.kwargs["query"] == "ê¹€ì¹˜"
        assert len(result.items) == 1
```

### 3.4 E2E í…ŒìŠ¤íŠ¸

```typescript
// e2e/core-loop.spec.ts

import { test, expect } from '@playwright/test';

test.describe('Core Loop', () => {
  test.beforeEach(async ({ page }) => {
    // í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìë¡œ ë¡œê·¸ì¸
    await page.goto('/login');
    await page.fill('[name="email"]', 'test@example.com');
    await page.fill('[name="password"]', 'password123');
    await page.click('button[type="submit"]');
    await expect(page).toHaveURL('/');
  });

  test('complete core loop: search -> cook -> feedback', async ({ page }) => {
    // 1. ë ˆì‹œí”¼ ê²€ìƒ‰
    await page.goto('/recipes/search');
    await page.fill('[name="query"]', 'ê¹€ì¹˜ì°Œê°œ');
    await page.click('button[type="submit"]');

    await expect(page.locator('[data-testid="recipe-card"]')).toHaveCount(
      { min: 1 }
    );

    // 2. ë ˆì‹œí”¼ ìƒì„¸ ë³´ê¸°
    await page.click('[data-testid="recipe-card"]:first-child');
    await expect(page.locator('h1')).toContainText('ê¹€ì¹˜ì°Œê°œ');

    // 3. ë ˆì‹œí”¼ë¶ì— ì €ì¥
    await page.click('[data-testid="save-to-cookbook"]');
    await expect(page.locator('[data-testid="toast"]')).toContainText('ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤');

    // 4. ì¡°ë¦¬ ì‹œì‘
    await page.click('[data-testid="start-cooking"]');
    await expect(page.locator('[data-testid="cooking-mode"]')).toBeVisible();

    // 5. ë‹¨ê³„ ì§„í–‰
    const totalSteps = await page.locator('[data-testid="total-steps"]').textContent();
    for (let i = 1; i < parseInt(totalSteps!); i++) {
      await page.click('[data-testid="next-step"]');
    }

    // 6. ì¡°ë¦¬ ì™„ë£Œ
    await page.click('[data-testid="complete-cooking"]');
    await expect(page.locator('[data-testid="feedback-form"]')).toBeVisible();

    // 7. í”¼ë“œë°± ì…ë ¥
    await page.click('[data-testid="taste-rating-4"]');
    await page.click('[data-testid="difficulty-rating-3"]');
    await page.fill('[name="feedbackText"]', 'ë§›ìˆì—ˆì–´ìš”! ë‹¤ìŒì—” ì†Œê¸ˆì„ ì¡°ê¸ˆ ì¤„ì—¬ë³¼ê²Œìš”.');
    await page.click('[data-testid="submit-feedback"]');

    // 8. AI ë³´ì • ê²°ê³¼ í™•ì¸
    await expect(page.locator('[data-testid="adjustment-status"]')).toContainText('ë³´ì • ì™„ë£Œ');
  });
});
```

---

## 4. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

### 4.1 ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

```mermaid
flowchart LR
    subgraph Development["ê°œë°œ"]
        DEV[ê°œë°œì]
        MIG[ë§ˆì´ê·¸ë ˆì´ì…˜ ì‘ì„±]
    end

    subgraph Testing["í…ŒìŠ¤íŠ¸"]
        LOCAL[ë¡œì»¬ í…ŒìŠ¤íŠ¸]
        STAGE[Staging ì ìš©]
    end

    subgraph Production["í”„ë¡œë•ì…˜"]
        BACKUP[ë°±ì—…]
        APPLY[ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš©]
        VERIFY[ê²€ì¦]
        ROLLBACK[ë¡¤ë°± ì¤€ë¹„]
    end

    DEV --> MIG
    MIG --> LOCAL
    LOCAL --> STAGE
    STAGE --> BACKUP
    BACKUP --> APPLY
    APPLY --> VERIFY
    VERIFY -.->|ì‹¤íŒ¨ ì‹œ| ROLLBACK
```

### 4.2 ë§ˆì´ê·¸ë ˆì´ì…˜ ê·œì¹™

```python
# migrations/versions/2024_01_01_add_recipe_rating.py

"""Add avg_rating column to recipes table

Revision ID: a1b2c3d4e5f6
Revises: previous_revision
Create Date: 2024-01-01 00:00:00.000000
"""

from alembic import op
import sqlalchemy as sa

# revision identifiers
revision = 'a1b2c3d4e5f6'
down_revision = 'previous_revision'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # 1. ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ (NULL í—ˆìš©ìœ¼ë¡œ ì‹œì‘)
    op.add_column(
        'recipes',
        sa.Column('avg_rating', sa.Numeric(2, 1), nullable=True)
    )

    # 2. ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (ë°°ì¹˜ ì²˜ë¦¬)
    op.execute("""
        UPDATE recipes r
        SET avg_rating = (
            SELECT COALESCE(AVG(f.taste_rating), 0)
            FROM cooking_feedbacks f
            JOIN cookbook_recipes cr ON f.cookbook_recipe_id = cr.id
            WHERE cr.original_recipe_id = r.id
        )
    """)

    # 3. NOT NULL ì œì•½ ì¶”ê°€ (í•„ìš”ì‹œ)
    op.alter_column(
        'recipes',
        'avg_rating',
        nullable=False,
        server_default='0'
    )

    # 4. ì¸ë±ìŠ¤ ì¶”ê°€ (CONCURRENTLYëŠ” ì§ì ‘ SQLë¡œ)
    op.execute("""
        CREATE INDEX CONCURRENTLY idx_recipes_avg_rating
        ON recipes(avg_rating DESC)
        WHERE is_active = true
    """)


def downgrade() -> None:
    op.execute("DROP INDEX IF EXISTS idx_recipes_avg_rating")
    op.drop_column('recipes', 'avg_rating')
```

### 4.3 Zero-Downtime ë§ˆì´ê·¸ë ˆì´ì…˜

```mermaid
flowchart TB
    subgraph Phase1["Phase 1: ì¤€ë¹„"]
        P1A[ìƒˆ ì»¬ëŸ¼ ì¶”ê°€<br/>NULL í—ˆìš©]
        P1B[ì‹ ê·œ ì½”ë“œì—ì„œ<br/>ì–‘ìª½ ì»¬ëŸ¼ ì“°ê¸°]
    end

    subgraph Phase2["Phase 2: ë§ˆì´ê·¸ë ˆì´ì…˜"]
        P2A[ë°±ê·¸ë¼ìš´ë“œ<br/>ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜]
        P2B[ê²€ì¦]
    end

    subgraph Phase3["Phase 3: ì „í™˜"]
        P3A[ì½ê¸°ë¥¼<br/>ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì „í™˜]
        P3B[NOT NULL<br/>ì œì•½ ì¶”ê°€]
    end

    subgraph Phase4["Phase 4: ì •ë¦¬"]
        P4A[ê¸°ì¡´ ì»¬ëŸ¼<br/>ì“°ê¸° ì œê±°]
        P4B[ê¸°ì¡´ ì»¬ëŸ¼<br/>ì‚­ì œ]
    end

    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
```

---

## 5. ë°ì´í„° ê´€ê³„ë„ (ER Diagram)

### 5.1 ì „ì²´ ER ë‹¤ì´ì–´ê·¸ë¨

```mermaid
erDiagram
    %% User Domain
    USERS ||--|| USER_PROFILES : has
    USERS ||--o{ OAUTH_ACCOUNTS : has
    USERS ||--o{ TASTE_PREFERENCES : has
    USERS ||--o{ COOKBOOKS : owns

    %% Recipe Domain (Crawling í¬í•¨)
    RECIPES ||--o{ RECIPE_SOURCES : crawled_from
    RECIPES ||--o{ RECIPE_SCORE_HISTORY : tracks
    RECIPES ||--o{ INGREDIENTS : contains
    RECIPES ||--o{ COOKING_STEPS : has
    RECIPES ||--o{ RECIPE_TAGS : tagged
    TAGS ||--o{ RECIPE_TAGS : applied

    %% Cookbook Domain
    COOKBOOKS ||--o{ COOKBOOK_RECIPES : contains
    COOKBOOK_RECIPES ||--o{ RECIPE_VERSIONS : versions
    COOKBOOK_RECIPES ||--o{ COOKING_FEEDBACKS : receives
    COOKBOOK_RECIPES ||--o{ COOKING_HISTORIES : logs
    COOKBOOK_RECIPES }o--|| RECIPES : references

    %% AI Domain
    COOKING_FEEDBACKS ||--o{ ADJUSTMENT_REQUESTS : triggers
    ADJUSTMENT_REQUESTS }o--o{ KNOWLEDGE_CHUNKS : uses
    QA_SESSIONS }o--o{ KNOWLEDGE_CHUNKS : references

    %% Entities
    USERS {
        uuid id PK
        string email UK
        string password_hash
        string name
        string role
        boolean is_active
        timestamp created_at
    }

    RECIPES {
        uuid id PK
        string title
        string author_name
        string author_channel
        string source_url
        string source_platform
        int cooking_time_minutes
        string difficulty
        decimal quality_score
        decimal popularity_score
        decimal exposure_score
        string content_hash
        decimal avg_rating
        boolean is_active
        boolean is_verified
        timestamp last_crawled_at
    }

    RECIPE_SOURCES {
        uuid id PK
        uuid recipe_id FK
        string platform
        string source_url UK
        string original_title
        bigint platform_view_count
        int platform_like_count
        int platform_comment_count
        jsonb raw_data
        timestamp first_discovered_at
        timestamp last_updated_at
    }

    RECIPE_SCORE_HISTORY {
        uuid id PK
        uuid recipe_id FK
        decimal quality_score
        decimal popularity_score
        decimal exposure_score
        text score_reason
        timestamp recorded_at
    }

    COOKBOOKS {
        uuid id PK
        uuid user_id FK
        string name
        boolean is_default
    }

    COOKBOOK_RECIPES {
        uuid id PK
        uuid cookbook_id FK
        uuid original_recipe_id FK
        jsonb adjusted_data
        int current_version
    }

    COOKING_FEEDBACKS {
        uuid id PK
        uuid cookbook_recipe_id FK
        int taste_rating
        int difficulty_rating
        text feedback_text
    }

    ADJUSTMENT_REQUESTS {
        uuid id PK
        uuid feedback_id FK
        string status
        jsonb output_result
    }
```

---

## ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|-----|------|----------|
| v1.0 | 2025.11.30 | ì´ˆê¸° ë¬¸ì„œ ì‘ì„± |

---

> **ì´ì „ ë¬¸ì„œ:** [5-1-6_INFRA.md](./5-1-6_INFRA.md) - ì¸í”„ë¼ ë° ë°°í¬
> **ë‹¤ìŒ ë¬¸ì„œ:** [5-1-8_OPERATIONS.md](./5-1-8_OPERATIONS.md) - ìš´ì˜ ê°€ì´ë“œ
